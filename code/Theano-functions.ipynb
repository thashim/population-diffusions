{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import manifold\n",
    "from sklearn.datasets import *\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.html.widgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if os.environ.get('THEANO_FLAGS') is not None:\n",
    "    del os.environ['THEANO_FLAGS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /cluster/thashim/.theanorc\n"
     ]
    }
   ],
   "source": [
    "%%writefile /cluster/thashim/.theanorc\n",
    "[global]\n",
    "floatX = float32\n",
    "device = gpu\n",
    "allow_gc = False\n",
    "[cuda]\n",
    "root = /usr/local/cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<CudaNdarrayType(float32, vector)>), HostFromGpu(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.777099 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in xrange(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Theano functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Tilogit(x):\n",
    "    return 1/(1+T.exp(-x))\n",
    "\n",
    "def T_relu_dprime(x):\n",
    "    return -1*Tilogit(x)*(1-Tilogit(x))\n",
    "\n",
    "def T_relu_prime(x):\n",
    "    return -1*Tilogit(x)\n",
    "\n",
    "def T_relu(x):\n",
    "    return -1*T.log(1+T.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Tdrift_relu_prime(x_p, w, b, g):\n",
    "    linterm = g*T_relu_dprime(T.dot(w,x_p)+b)\n",
    "    return T.dot(w.T,linterm)\n",
    "\n",
    "def Tsimul_relu_prime(z, x_p, w, b, g, dt):\n",
    "    return x_p + Tdrift_relu_prime(x_p,w,b,g)*dt + T.sqrt(dt)*z\n",
    "\n",
    "def Tpot_relu_prime(x_p, w, b, g):\n",
    "    linterm = g*T_relu_prime(T.dot(w,x_p)+b)\n",
    "    return T.sum(linterm, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Tdrift_relu(x_p, w, b, g):\n",
    "    linterm = g*T_relu_prime(T.dot(w,x_p)+b)\n",
    "    return T.dot(w.T,linterm)\n",
    "\n",
    "def Tsimul_relu(z, x_p, w, b, g, dt):\n",
    "    return x_p + Tdrift_relu(x_p,w,b,g)*dt + T.sqrt(dt)*z\n",
    "\n",
    "def Tpot_relu(x_p, w, b, g):\n",
    "    linterm = g*T_relu(T.dot(w,x_p)+b)\n",
    "    return T.sum(linterm, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Tpot_quad(x_p, w, b, g):\n",
    "    sqdist = (x_p ** 2).sum(0).reshape((1, x_p.shape[1])) + (w ** 2).sum(1).reshape((w.shape[0], 1)) - 2 * T.dot(w,x_p)\n",
    "    kernest = T.exp(-sqdist / b)*g\n",
    "    return T.sum(kernest, 0)\n",
    "\n",
    "def Tdrift_quad(x_p, w, b, g):\n",
    "    ksum = T.sum(Tpot_quad(x_p, w, b, g))\n",
    "    driftterm = theano.gradient.grad(ksum, x_p)\n",
    "    return driftterm\n",
    "\n",
    "def Tsimul_quad(z, x_p, w, b, g, dt):\n",
    "    return x_p + Tdrift_quad(x_p,w,b,g)*dt + T.sqrt(dt)*z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Tpot_lin(x_p, w, b, g):\n",
    "    linterm = g*T.dot(w, x_p)\n",
    "    return T.sum(linterm, 0)\n",
    "\n",
    "def Tdrift_lin(x_p, w, b, g):\n",
    "    ksum = T.sum(Tpot_lin(x_p,w,b,g))\n",
    "    driftterm = theano.gradient.grad(ksum, x_p)\n",
    "    return driftterm\n",
    "\n",
    "def Tsimul_lin(z, x_p, w, b, g, dt):\n",
    "    return x_p + Tdrift_lin(x_p,w,b,g)*dt + T.sqrt(dt)*z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Tpot_ou(x_p, w, b, g):\n",
    "    sqdist = (x_p ** 2).sum(0).reshape((1, x_p.shape[1])) + (w ** 2).sum(1).reshape((w.shape[0], 1)) - 2 * T.dot(w,x_p)\n",
    "    return T.sum(-sqdist * g, 0)\n",
    "\n",
    "def Tdrift_ou(x_p, w, b, g):\n",
    "    ksum = T.sum(Tpot_ou(x_p, w, b, g))\n",
    "    driftterm = theano.gradient.grad(ksum, x_p)\n",
    "    return driftterm\n",
    "\n",
    "def Tsimul_ou(z, x_p, w, b, g, dt):\n",
    "    return x_p + Tdrift_ou(x_p,w,b,g)*dt + T.sqrt(dt)*z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import numpy as np\n",
    "x_pt = np.array([[0,0],[1,1]]).T\n",
    "wt = np.array([[-1,-1],[2,2],[3,3]])\n",
    "gi = np.array([1,1,1])[:,np.newaxis]*10\n",
    "bi = np.array([1,1,1])[:,np.newaxis]\n",
    "zi = np.random.randn(2,2)*0\n",
    "\n",
    "quad_pot_val = Tpot_quad(xi, w, b, g)\n",
    "quad_pot_test = theano.function(inputs=[xi, w, b, g],outputs=quad_pot_val, allow_input_downcast=True)\n",
    "quad_pot_test(x_pt, wt, bi, gi)\n",
    "\n",
    "quad_drift_val = Tdrift_quad(xi, w, b, g)\n",
    "quad_drift_test = theano.function(inputs=[xi, w, b, g],outputs=quad_drift_val, allow_input_downcast=True)\n",
    "quad_drift_test(x_pt, wt, bi, gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theano variables\n",
    "n_steps = T.iscalar('n_steps')\n",
    "dt = T.fscalar('dt')\n",
    "xi = T.matrix(\"xi\")\n",
    "z = T.tensor3(\"z\")\n",
    "zmat = T.matrix(\"zmat\")\n",
    "w = T.matrix(\"w\")\n",
    "b = T.TensorType(dtype='float32',broadcastable=(False,True))('b')\n",
    "g = T.TensorType(dtype='float32',broadcastable=(False,True))('g')\n",
    "err = T.matrix(\"err\")\n",
    "\n",
    "def theano_meta_factory(sim_fn, drift_fn, pot_fn, name):\n",
    "    return {'potential':pot_factory(pot_fn),\n",
    "           'drift':drift_factory(drift_fn),\n",
    "           'trajectory':em_traj_factory(sim_fn),\n",
    "           'simulate':em_final_factory(sim_fn),\n",
    "           'backprop':em_lop_factory(sim_fn),\n",
    "           'potential_grad':em_pot_factory(pot_fn),\n",
    "           'name':name}\n",
    "\n",
    "def drift_factory(drift_fn):\n",
    "    drift_val = drift_fn(xi, w, b, g)\n",
    "    return theano.function(inputs=[xi, w, b, g], outputs=drift_val, allow_input_downcast=True,on_unused_input='ignore')\n",
    "\n",
    "def pot_factory(pot_fn):\n",
    "    pot_val = pot_fn(xi, w, b, g)\n",
    "    return theano.function(inputs=[xi, w, b, g], outputs=pot_val, allow_input_downcast=True,on_unused_input='ignore')\n",
    "\n",
    "def em_traj_factory(sim_fn):\n",
    "    result, updates = theano.scan(fn = sim_fn, sequences = z, outputs_info = xi, non_sequences = [w, b, g, dt], n_steps = n_steps)\n",
    "    em_traj_fun = theano.function(inputs = [z, xi, w, b, g, dt, n_steps], outputs= result, updates=updates, allow_input_downcast=True,on_unused_input='ignore')\n",
    "    return em_traj_fun\n",
    "\n",
    "def em_final_factory(sim_fn):\n",
    "    result, updates = theano.scan(fn = sim_fn, sequences = z, outputs_info = xi, non_sequences = [w, b, g, dt], n_steps = n_steps)\n",
    "    res_final = result[-1]\n",
    "    em_final_fun = theano.function(inputs = [z, xi, w, b, g, dt, n_steps], outputs=res_final, updates=updates, allow_input_downcast=True,on_unused_input='ignore')\n",
    "    return em_final_fun\n",
    "\n",
    "def em_lop_factory(sim_fn):\n",
    "    result, updates = theano.scan(fn = sim_fn, sequences = z, outputs_info = xi, non_sequences = [w, b, g, dt], n_steps = n_steps)\n",
    "    gradval = theano.gradient.Lop(T.flatten(result[-1]), [w, b, g], T.flatten(err), disconnected_inputs='warn')\n",
    "    gradfun = theano.function(inputs = [err, z, xi, w, b, g, dt, n_steps], outputs=gradval, updates=updates, allow_input_downcast=True,on_unused_input='ignore')\n",
    "    return gradfun\n",
    "\n",
    "def em_pot_factory(pot_fn):\n",
    "    pot_val = T.sum(pot_fn(xi, w, b, g))\n",
    "    gradval = theano.gradient.grad(pot_val, [w, b, g], disconnected_inputs='warn')\n",
    "    potfun = theano.function(inputs = [xi, w, b, g], outputs = pot_val, allow_input_downcast=True,on_unused_input='ignore')\n",
    "    potgrad = theano.function(inputs = [xi, w, b, g], outputs = gradval, allow_input_downcast=True,on_unused_input='ignore')\n",
    "    return potgrad, potfun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relu_pack = theano_meta_factory(Tsimul_relu,Tdrift_relu,Tpot_relu, 'ramp potential')\n",
    "local_pack = theano_meta_factory(Tsimul_quad,Tdrift_quad,Tpot_quad, 'local potential')\n",
    "logit_pack = theano_meta_factory(Tsimul_relu_prime,Tdrift_relu_prime,Tpot_relu_prime, 'logit potential')\n",
    "ou_pack = theano_meta_factory(Tsimul_ou, Tdrift_ou, Tpot_ou, 'Orstein-Uhlenbeck potential')\n",
    "lin_pack = theano_meta_factory(Tsimul_lin, Tdrift_lin, Tpot_lin, 'Linear potential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "class observed:\n",
    "    def __init__(self, p_init, p_out):\n",
    "        self.p_init=p_init\n",
    "        self.p_out=p_out\n",
    "class hyperpars:\n",
    "    def __init__(self, NS, eps, sd, sdkern, dt, time):\n",
    "        self.eps=eps\n",
    "        self.NS=NS\n",
    "        self.sd=sd\n",
    "        self.sdkern=sdkern\n",
    "        self.dt=dt\n",
    "        self.time=time\n",
    "class parset:\n",
    "    def __init__(self, K, D, potin=relu_pack, scale=1, muzero=None):\n",
    "        if muzero is None:\n",
    "            muzero = np.zeros(D)\n",
    "        self.potin=potin\n",
    "        self.W_matrix=np.random.randn(K,D)*scale\n",
    "        if 'local' not in potin['name']:\n",
    "            offset = np.dot(self.W_matrix,muzero)\n",
    "            self.b_vec=np.random.uniform(low=-1,high=1,size=K) - offset\n",
    "        else:\n",
    "            self.b_vec=np.ones(K)*5.0\n",
    "        self.g_vec=np.zeros(K)\n",
    "        self.W_sqsum=np.ones(self.W_matrix.shape)\n",
    "        self.b_sqsum=np.ones(self.b_vec.shape)\n",
    "        self.g_sqsum=np.ones(self.g_vec.shape)\n",
    "        self.fvvec=[]\n",
    "        self.tvec=[]\n",
    "        self.tnow=0\n",
    "    \n",
    "    def gclip(self, grad, gmax=1e5):\n",
    "        g_new = []\n",
    "        for i in xrange(len(grad)):\n",
    "            vnorm = np.sqrt(np.sum(grad[i]**2.0))\n",
    "            sfactor = max(1, vnorm/gmax)\n",
    "            g_new.append(np.copy(grad[i]/sfactor))\n",
    "        return g_new\n",
    "        \n",
    "    def update(self, grad, eps_val, fv, tv, ada=1e-3):\n",
    "        self.W_sqsum = self.W_sqsum + eps_val*ada*grad[0]**2\n",
    "        self.b_sqsum = self.b_sqsum + eps_val*ada*grad[1]**2\n",
    "        self.g_sqsum = self.g_sqsum + eps_val*ada*grad[2]**2\n",
    "        self.W_matrix = self.W_matrix + eps_val*grad[0]/np.sqrt(self.W_sqsum)\n",
    "        self.b_vec = self.b_vec + eps_val*grad[1]/np.sqrt(self.b_sqsum)\n",
    "        self.g_vec = self.g_vec + eps_val*grad[2]/np.sqrt(self.g_sqsum)\n",
    "        self.fvvec.append(fv)\n",
    "        self.tnow=self.tnow+tv\n",
    "        self.tvec.append(self.tnow)\n",
    "    \n",
    "    def reset_ada(self):\n",
    "        self.W_sqsum=np.ones(self.W_matrix.shape)\n",
    "        self.b_sqsum=np.ones(self.b_vec.shape)\n",
    "        self.g_sqsum=np.ones(self.g_vec.shape)\n",
    "        \n",
    "    def copy(self):\n",
    "        parnew = parset(K=self.b_vec.shape[0],D=self.W_matrix.shape[1],potin=self.potin)\n",
    "        parnew.W_matrix = np.copy(self.W_matrix)\n",
    "        parnew.b_vec = np.copy(self.b_vec)\n",
    "        parnew.g_vec = np.copy(self.g_vec)\n",
    "        parnew.fvvec = copy.copy(self.fvvec)\n",
    "        parnew.tvec = copy.copy(self.tvec)\n",
    "        parnew.tnow = self.tnow\n",
    "        return parnew\n",
    "    \n",
    "    def plot(self, xpair, ypair):\n",
    "        xseq = np.linspace(xpair[0],xpair[1],num=50)\n",
    "        yseq = np.linspace(ypair[0],ypair[1],num=50)\n",
    "        plot_flow_pot(self.potin,xseq,yseq,self.W_matrix,self.b_vec,self.g_vec)\n",
    "        plot_flow_par(xseq,yseq,self.potin,self.W_matrix,self.b_vec,self.g_vec)\n",
    "            \n",
    "    def simulate(self, init, ns, time, dt, sd):\n",
    "        W_mat = self.W_matrix\n",
    "        b_v = self.b_vec[:,np.newaxis]\n",
    "        g_v = self.g_vec[:,np.newaxis]\n",
    "        num_steps = int(time/dt)\n",
    "        pp = p_samp(init,ns)\n",
    "        z = rng.randn(num_steps,pp.shape[0],pp.shape[1])*sd\n",
    "        return self.potin['simulate'](z, pp, W_mat, b_v, g_v, dt, num_steps)\n",
    "    \n",
    "class observed_list:\n",
    "    def __init__(self, p_list, t_list):\n",
    "        self.p_list=p_list\n",
    "        self.t_list=t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_flow_par(x,y,potfun,W,b,g):\n",
    "    u=np.zeros((x.shape[0],y.shape[0]))\n",
    "    v=np.zeros((x.shape[0],y.shape[0]))\n",
    "    nrm=np.zeros((x.shape[0],y.shape[0]))\n",
    "    for i in xrange(x.shape[0]):\n",
    "        ptv=np.vstack((np.full(y.shape[0],x[i]),y))\n",
    "        flowtmp=potfun['drift'](ptv,W,b[:,np.newaxis],g[:,np.newaxis])\n",
    "        u[:,i]=flowtmp[0,:]\n",
    "        v[:,i]=flowtmp[1,:]\n",
    "        nrm[:,i]=np.sqrt(np.sum(flowtmp**2.0,0))\n",
    "    #plt.quiver(x,y,u,v)\n",
    "    plt.streamplot(x,y,u,v,density=1.0,linewidth=3*nrm/np.max(nrm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_flow_pot(pot,x,y,W,b,g):\n",
    "    z=np.zeros((x.shape[0],y.shape[0]))\n",
    "    for i in xrange(x.shape[0]):\n",
    "        ptv=np.vstack((np.full(y.shape[0],x[i]),y))\n",
    "        flowtmp= pot['potential'](ptv,W,b[:,np.newaxis],g[:,np.newaxis])\n",
    "        z[:,i]=flowtmp\n",
    "    plt.pcolormesh(x,y,np.exp(z))\n",
    "    CS = plt.contour(x,y,z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_samp(p_in, num_samp):\n",
    "    repflag = p_in.shape[1] < num_samp\n",
    "    p_sub=np.random.choice(p_in.shape[1],size=num_samp,replace=repflag)\n",
    "    return np.copy(p_in[:,p_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-31b187d6798b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mget_grad_logp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano_pack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mfactr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mW_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mb_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mg_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def get_grad_logp(parin, samples, pp, burnin, theano_pack, dt, sd=np.sqrt(2)):\n",
    "    factr = np.shape(samples)[1]/float(np.shape(pp)[1])\n",
    "    W_mat = parin.W_matrix\n",
    "    b_v = parin.b_vec[:,np.newaxis]\n",
    "    g_v = parin.g_vec[:,np.newaxis]\n",
    "    num_steps = burnin\n",
    "    z = rng.randn(num_steps,pp.shape[0],pp.shape[1])*sd\n",
    "    #run chain forward, get result\n",
    "    result_final = theano_pack['simulate'](z, pp, W_mat, b_v, g_v, dt, num_steps)\n",
    "    #logp with respect to input samples\n",
    "    grad_pos = theano_pack['potential_grad'][0](samples,W_mat, b_v, g_v)\n",
    "    pos_fv = theano_pack['potential_grad'][1](samples,W_mat,b_v,g_v)\n",
    "    #logp with respect to contrastive divergence smaples\n",
    "    grad_neg = theano_pack['potential_grad'][0](result_final,W_mat, b_v, g_v)\n",
    "    neg_fv = theano_pack['potential_grad'][1](result_final,W_mat, b_v, g_v)\n",
    "    fv_tot = pos_fv - factr*neg_fv\n",
    "    dW = grad_pos[0]-grad_neg[0]*factr\n",
    "    db = np.squeeze(grad_pos[1]-grad_neg[1]*factr)\n",
    "    dg = np.squeeze(grad_pos[2]-grad_neg[2]*factr)\n",
    "    return [[dW, db, dg],-1*fv_tot, result_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_logp_theano(parin,samples, niter, stepsize,theano_pack,dt=0.01, burnin=10, ns=500, ctk=True, ada_val=0,sd=np.sqrt(2)):\n",
    "    for i in xrange(niter):\n",
    "        pp = p_samp(samples,ns)\n",
    "        t_start = time.clock()\n",
    "        gradin, fv_tot, result_final = get_grad_logp(parin, samples, pp, burnin, theano_pack, dt, sd=sd)\n",
    "        if ctk:\n",
    "            pp = result_final\n",
    "        parin.update(gradin,stepsize/np.shape(samples)[1],fv_tot,time.clock()-t_start, ada_val)\n",
    "    return parin, result_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_grad_marginal(parin,pp,p_target,theano_pack,time,dt,sd,sdkern,lossfun):\n",
    "    W_mat = parin.W_matrix\n",
    "    b_v = parin.b_vec[:,np.newaxis]\n",
    "    g_v = parin.g_vec[:,np.newaxis]\n",
    "    num_steps = int(time / float(dt))\n",
    "    z = rng.randn(num_steps,pp.shape[0],pp.shape[1])*sd\n",
    "    result_final = theano_pack['simulate'](z, pp, W_mat, b_v, g_v, dt, num_steps)\n",
    "    err_out, fval=lossfun(result_final,p_target,sdkern)\n",
    "    gall = theano_pack['backprop'](err_out, z, pp, W_mat, b_v, g_v, dt, num_steps)\n",
    "    gall[1]=np.squeeze(gall[1])\n",
    "    gall[2]=np.squeeze(gall[2])\n",
    "    return gall, fval, result_final, err_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_grad_theano(datin,parin,hpars,maxit,theano_pack,lossfun,tau=0,burnin=10,ctk=True,debug=True,ada_val=0):\n",
    "    if debug:\n",
    "        f = FloatProgress(min=0, max=maxit)\n",
    "        display(f)\n",
    "    num_samp = hpars.NS\n",
    "    for i in xrange(maxit):\n",
    "        pp = p_samp(datin.p_init, num_samp)\n",
    "        pneg = pp\n",
    "        time_start = time.clock()\n",
    "        gall, fval, result_final, err_out = get_grad_marginal(parin, pp, datin.p_out, theano_pack, hpars.time, hpars.dt, hpars.sd, hpars.sdkern,lossfun)\n",
    "        if tau is not 0: #entropic regularization below.\n",
    "            gall_logp, fv_logp, result_logp = get_grad_logp(parin, datin.p_out, pneg, burnin, theano_pack, hpars.dt, hpars.sd)\n",
    "            if ctk:\n",
    "                pneg = result_logp\n",
    "            for j in xrange(3):\n",
    "                gall[j] = gall[j]*(1-tau) + gall_logp[j]*tau\n",
    "        parin.update(gall,hpars.eps,fval,time.clock()-time_start,ada_val/num_samp)\n",
    "        if np.isneginf(fval):\n",
    "            break\n",
    "        if debug:\n",
    "            f.value = i\n",
    "    if debug:\n",
    "        print(fval)\n",
    "        plt.figure(1)\n",
    "        plt.plot(parin.fvvec)\n",
    "        plt.figure(2)\n",
    "        plt.scatter(result_final[0,:],result_final[1,:],c='red')\n",
    "        plt.scatter(datin.p_out[0,:],datin.p_out[1,:])\n",
    "        plt.quiver(result_final[0,:],result_final[1,:],err_out[0,:],err_out[1,:])\n",
    "    return parin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_grad_theano_list(datin_list,parin,hpars,maxit,theano_pack,lossfun, tau=0, burnin=10,ctk=True,delta=False,debug=True,ada_val=0):\n",
    "    if debug:\n",
    "        f = FloatProgress(min=0, max=maxit)\n",
    "        display(f)\n",
    "    num_samp = hpars.NS\n",
    "    dlast = datin_list.p_list[len(datin_list.t_list)-1]\n",
    "    for i in xrange(maxit):\n",
    "        pneg = p_samp(dlast, num_samp)\n",
    "        db = np.zeros(parin.b_vec.shape)\n",
    "        dg = np.zeros(parin.b_vec.shape)\n",
    "        dW = np.zeros(parin.W_matrix.shape)\n",
    "        fv_tmp = 0\n",
    "        time_start = time.clock()\n",
    "        for j in xrange(len(datin_list.t_list)-1):\n",
    "            if not delta:\n",
    "                t_cur = datin_list.t_list[j+1] - datin_list.t_list[0]\n",
    "                dat_cur = datin_list.p_list[j+1]\n",
    "                dat_init = datin_list.p_list[0]\n",
    "            else:\n",
    "                t_cur = datin_list.t_list[j+1]-datin_list.t_list[j]\n",
    "                dat_cur = datin_list.p_list[j+1]\n",
    "                dat_init = datin_list.p_list[j]\n",
    "            pp = p_samp(dat_init, num_samp)\n",
    "            gall, fval, result_final, err_out = get_grad_marginal(parin, pp, dat_cur, theano_pack, t_cur, hpars.dt, hpars.sd, hpars.sdkern,lossfun)\n",
    "            dW = dW + gall[0]\n",
    "            db = db + gall[1]\n",
    "            dg = dg + gall[2]\n",
    "            fv_tmp = fv_tmp + fval\n",
    "        gnew = [dW, db, dg]\n",
    "        if tau is not 0: #entropic regularization below.\n",
    "            gall_logp, fv_logp, result_logp = get_grad_logp(parin, dlast, pneg, burnin, theano_pack, hpars.dt, hpars.sd)\n",
    "            if ctk:\n",
    "                pneg = result_logp\n",
    "            for j in xrange(3):\n",
    "                gnew[j] = gnew[j]*(1-tau) + gall_logp[j]*tau\n",
    "        if np.isneginf(fv_tmp):\n",
    "            break\n",
    "        parin.update(gnew,hpars.eps,fv_tmp,time.clock()-time_start,ada_val/num_samp)\n",
    "        if debug:\n",
    "            f.value = i\n",
    "    if debug:\n",
    "        print(fval)\n",
    "        plt.figure(1)\n",
    "        plt.plot(parin.fvvec)\n",
    "        for j in xrange(len(datin_list.t_list)-1):\n",
    "            plt.figure(j+2)\n",
    "            t_cur = datin_list.t_list[j+1]-datin_list.t_list[j]\n",
    "            dat_cur = datin_list.p_list[j+1]\n",
    "            dat_init = datin_list.p_list[j]\n",
    "            W_mat = parin.W_matrix\n",
    "            b_v = parin.b_vec[:,np.newaxis]\n",
    "            g_v = parin.g_vec[:,np.newaxis]\n",
    "            num_steps = int(t_cur / float(hpars.dt))\n",
    "            z = rng.randn(num_steps,dat_init.shape[0],dat_init.shape[1])*hpars.sd\n",
    "            result_final = theano_pack['simulate'](z, dat_init, W_mat, b_v, g_v, hpars.dt, num_steps)\n",
    "            plt.scatter(result_final[0],result_final[1],c='red')\n",
    "            plt.scatter(dat_cur[0],dat_cur[1])\n",
    "    return parin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein loss stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkmat(mat):\n",
    "    is_finite = np.all(np.isfinite(mat))\n",
    "    is_nontrivial = np.ptp(mat)>1e-5\n",
    "    return is_finite and is_nontrivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import utils\n",
    "import hungarian\n",
    "\n",
    "def wasserstein_error(p_pred, p_true, sdkern):\n",
    "    ptrue_resamp = p_samp(p_true, p_pred.shape[1])\n",
    "    distsq = get_dist(p_pred,ptrue_resamp)\n",
    "    #matching = utils.linear_assignment_._hungarian(distsq)\n",
    "    distsq[np.isposinf(distsq)]=1e5\n",
    "    if checkmat(distsq):\n",
    "        matching = hungarian.lap(distsq)\n",
    "    else:\n",
    "        matching = [np.arange(p_pred.shape[1]), np.arange(p_pred.shape[1])]\n",
    "    #m1=matching[0]\n",
    "    m1=np.arange(len(matching[0]))\n",
    "    #m2=matching[1]\n",
    "    m2=matching[0]\n",
    "    spts = p_pred[:,m1]\n",
    "    dlts = ptrue_resamp[:,m2]-spts\n",
    "    errs = np.sum(dlts**2.0,0)\n",
    "    return dlts, -1*np.sum(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist(yt, ytrue):\n",
    "    ytnorm = np.sum(yt**2,0)\n",
    "    ytruenorm = np.sum(ytrue**2,0)\n",
    "    dotprod = np.dot(yt.T,ytrue)\n",
    "    return np.add.outer(ytnorm,ytruenorm) - 2*dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sinkhorn(M, lamb, r, c, maxit=100):\n",
    "    #Mp = np.array(M,dtype=np.float128)\n",
    "    Mp=M\n",
    "    K = np.exp(-lamb*(Mp))#-np.min(Mp)))\n",
    "    rp = np.copy(r)\n",
    "    cp = np.copy(c)\n",
    "    for i in xrange(maxit):\n",
    "        cp = 1.0/np.dot(rp,K)\n",
    "        rp = 1.0/np.dot(K,cp)\n",
    "    kn = rp[:,np.newaxis]*K*cp\n",
    "    return cp, rp, kn#np.dot(np.dot(np.diag(rp),K),np.diag(cp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sinkhorn_error(p_pred, p_true, sdkern, rep=0, numit=10):\n",
    "    if sdkern is None:\n",
    "        sdkern = 10.0\n",
    "    ptrue_resamp = p_samp(p_true, p_pred.shape[1])\n",
    "    distsq = get_dist(p_pred,ptrue_resamp)\n",
    "    sko = sinkhorn(distsq, sdkern, np.ones(distsq.shape[0]),np.ones(distsq.shape[1]),numit)[2]\n",
    "    sko = sko / sko.sum(axis=1,keepdims=True)\n",
    "    if np.all(np.isfinite(sko)):\n",
    "        targ = np.dot(ptrue_resamp,np.transpose(sko))\n",
    "        dlts = targ - p_pred\n",
    "        return dlts, -1*np.sum(dlts**2.0)\n",
    "    else:\n",
    "        if rep < 10:\n",
    "            return sinkhorn_error(p_pred, p_true, sdkern/2.0, rep=rep+1)\n",
    "        else:\n",
    "            return p_pred, -float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sinkhorn_hiprec(M, lamb, r, c, maxit=100):\n",
    "    Mp = np.array(M,dtype=np.float128)\n",
    "    K = np.exp(-lamb*(Mp-np.min(Mp)))\n",
    "    rp = np.copy(r)\n",
    "    cp = np.copy(c)\n",
    "    for i in xrange(maxit):\n",
    "        cp = 1.0/np.dot(rp,K)\n",
    "        rp = 1.0/np.dot(K,cp)\n",
    "    kn = rp[:,np.newaxis]*K*cp\n",
    "    return cp, rp, kn#np.dot(np.dot(np.diag(rp),K),np.diag(cp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sinkhorn_error_hiprec(p_pred, p_true, sdkern, rep=0, numit=10):\n",
    "    if sdkern is None:\n",
    "        sdkern = 100.0\n",
    "    ptrue_resamp = p_samp(p_true, p_pred.shape[1])\n",
    "    distsq = get_dist(p_pred,ptrue_resamp)\n",
    "    sko = sinkhorn_hiprec(distsq, sdkern, np.ones(distsq.shape[0]),np.ones(distsq.shape[1]),numit)[2]\n",
    "    sko = sko / sko.sum(axis=1,keepdims=True)\n",
    "    if np.all(np.isfinite(sko)):\n",
    "        targ = np.dot(ptrue_resamp,np.transpose(sko))\n",
    "        dlts = targ - p_pred\n",
    "        return dlts, -1*np.sum(dlts**2.0)\n",
    "    else:\n",
    "        if rep < 10:\n",
    "            return sinkhorn_error(p_pred, p_true, sdkern/2.0, rep=rep+1)\n",
    "        else:\n",
    "            return p_pred, -float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autorun script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale_par(par,snew):\n",
    "    parnew = par.copy()\n",
    "    parnew.g_vec = np.copy(par.g_vec) * snew**2.0 / 2.0\n",
    "    return parnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pack_sim(parin, pp, t, dt, sd, theano_pack):\n",
    "    num_steps = int(t / float(dt))\n",
    "    z = rng.randn(num_steps,pp.shape[0],pp.shape[1])*sd\n",
    "    return theano_pack['simulate'](z, pp, parin.W_matrix, parin.b_vec, parin.g_vec, dt, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_all(data_in, time_in,theano_pack,tau=0,sdin = 1.0, Knum=100, dtin=0.01,burnin=100,lossfun=sinkhorn_error, n1=5, n2=10, eps_base=0.01, scale_base=1, debug=True):\n",
    "    np.random.seed(0)\n",
    "    data_last = data_in[-1]\n",
    "    time_last = time_in[-1]\n",
    "    NS = data_last.shape[1]\n",
    "    best_err = -1e8\n",
    "    best_par = None\n",
    "    bct = 50\n",
    "    powr = 2.0\n",
    "    for j in xrange(n1):\n",
    "        init_par=parset(potin=theano_pack,K=Knum,D=data_last.shape[0],scale=1.0)\n",
    "        init_par, p_mat = run_logp_theano(init_par,data_last,400,eps_base/float(powr**j),theano_pack,dt=(time_last)/bct, burnin=bct, ns=NS, ctk=False, ada_val=0.0)\n",
    "        grad, errval = lossfun(p_mat, data_last, None)\n",
    "        if debug:\n",
    "            print errval\n",
    "        if errval > best_err:\n",
    "            best_par = init_par.copy()\n",
    "            best_err = errval\n",
    "    fvbase = -1e8\n",
    "    best_out = None\n",
    "    best_eps = None\n",
    "    ada_2 = 1/100.0\n",
    "    for j in xrange(n2):\n",
    "        #h_par= rescale_par(best_par, sdin)\n",
    "        h_par = best_par.copy()\n",
    "        epsin = eps_base/(10.0*float(powr**j))*scale_base\n",
    "        #print epsin\n",
    "        h_hyp=hyperpars(NS=NS,eps=epsin,sd=sdin,sdkern=None,dt=dtin,time=time_in[1]-time_in[0])\n",
    "        if len(time_in) is 2:\n",
    "            h_dat=observed(data_in[0], data_in[1])\n",
    "            parout = run_grad_theano(h_dat,h_par,h_hyp,100,theano_pack,tau=tau,burnin=burnin,lossfun=lossfun,ada_val=ada_2, debug=False)\n",
    "        else:\n",
    "            hl_dat=observed_list(data_in,time_in)\n",
    "            parout = run_grad_theano_list(hl_dat,h_par,h_hyp,100,theano_pack,tau=tau,burnin=burnin,lossfun=lossfun,ada_val=ada_2, debug=False, delta=True)\n",
    "        if debug:\n",
    "            print (parout.fvvec[-1], epsin)\n",
    "        #pred_output = pack_sim(parout, data_in[0], data_last, )\n",
    "        if parout.fvvec[-1] > fvbase:\n",
    "            best_eps = epsin\n",
    "            best_out = parout.copy()\n",
    "            fvbase = parout.fvvec[-1]\n",
    "    #best_out= rescale_par(best_par, sdin)\n",
    "    best_out2 = best_par.copy()\n",
    "    epsin = best_eps\n",
    "    h_hyp=hyperpars(NS=NS,eps=epsin,sd=sdin,sdkern=None,dt=dtin,time=time_in[1]-time_in[0])\n",
    "    if len(time_in) is 2:\n",
    "        h_dat=observed(data_in[0], data_in[1])\n",
    "        best_out2 = run_grad_theano(h_dat,best_out2,h_hyp,500,theano_pack,tau=tau,burnin=burnin,lossfun=lossfun,ada_val=ada_2, debug=debug)\n",
    "    else:\n",
    "        hl_dat=observed_list(data_in,time_in)\n",
    "        best_out2 = run_grad_theano_list(hl_dat,best_out2,h_hyp,500,theano_pack,tau=tau,burnin=burnin,lossfun=lossfun,ada_val=ada_2, debug=debug, delta=True)\n",
    "    if not np.isfinite(best_out2.fvvec[-1]):\n",
    "        best_out2 = best_out\n",
    "    return best_out2, best_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and simulation related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euler_maruyama_dist(p, flow, dt, t, sd):\n",
    "    pp = np.copy(p)\n",
    "    n = int(t/dt)\n",
    "    sqrtdt = np.sqrt(dt)\n",
    "    for i in xrange(n):\n",
    "        drift = flow(pp)\n",
    "        pp = pp + drift*dt + np.random.normal(scale=sd,size=p.shape)*sqrtdt\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_flow(x,y,fun,ladj=5):\n",
    "    u=np.zeros((x.shape[0],y.shape[0]))\n",
    "    v=np.zeros((x.shape[0],y.shape[0]))\n",
    "    nrm=np.zeros((x.shape[0],y.shape[0]))\n",
    "    for i in xrange(x.shape[0]):\n",
    "        ptv=np.vstack((np.full(y.shape[0],x[i]),y))\n",
    "        flowtmp=fun(ptv)\n",
    "        u[:,i]=flowtmp[0,:]\n",
    "        v[:,i]=flowtmp[1,:]\n",
    "        nrm[:,i]=np.sqrt(np.sum(flowtmp**2.0,0))\n",
    "    plt.streamplot(x,y,u,v,density=1.0,linewidth=ladj*nrm/np.max(nrm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fminbound\n",
    "def error_term(yt, ytrue, kern_sig, minv = 1e-4):\n",
    "    distsq = get_dist(yt,ytrue)\n",
    "    d=yt.shape[0]\n",
    "    if kern_sig is None:\n",
    "        train_size = int(0.2*yt.shape[1])+1\n",
    "        indices = np.random.permutation(yt.shape[1])\n",
    "        training_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "        training, test = yt[:,training_idx], yt[:,test_idx]\n",
    "        dist_train = get_dist(training,test)\n",
    "        spo=fminbound(error_from_dmat, x1=minv, x2=max(np.max(dist_train),4.0*minv)/2.0, args=(dist_train, d), full_output=True)\n",
    "        kern_sig = spo[0]\n",
    "    expterm = np.exp(-distsq/(2*kern_sig))/kern_sig**(d/2.0)\n",
    "    esum = np.sum(expterm,0)\n",
    "    #print esum.shape\n",
    "    errweight = expterm/esum\n",
    "    grad_err = np.zeros(yt.shape)\n",
    "    for i in xrange(errweight.shape[0]):\n",
    "        grad_err[:,i]=np.sum(-2*(yt[:,i][:,np.newaxis]-ytrue)/kern_sig*errweight[i,],1)\n",
    "    return grad_err, np.sum(np.log(esum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_from_dmat(kern_sig, distsq, d):\n",
    "    expterm = np.exp(-distsq/(2*kern_sig))/kern_sig**(d/2.0)\n",
    "    fv = -1*np.sum(np.log(np.sum(expterm,0)))\n",
    "    #print kern_sig, fv\n",
    "    return fv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
